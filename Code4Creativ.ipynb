{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb0031fb-ebd2-4680-aeb4-35d827a68c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import TensorDataset\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9a19d-2f36-4538-afc9-f653abba1d06",
   "metadata": {},
   "source": [
    "### TTCT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d322635-2365-47cb-8e40-c41475e6ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kodetu_creat_test = pd.read_csv('TTCT_AUT_LOG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ab777-5023-478c-80fb-255b43c45b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "kodetu_creat_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d465e431-9b67-4a6a-baae-5ff10e07313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_columns = kodetu_creat_test[['TTCT_PRE_fluency','TTCT_PRE_elaboration','TTCT_PRE_flexibility']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8fec22-e7c1-4b77-af70-3a3f7b61f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kodetu_creat_test[(filtered_columns != '.').all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18fa48-100a-488f-97f8-3ef3faefd213",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kodetu_creat_test[(filtered_columns != '.').all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fdda5-36e4-4682-ac2d-67b1f4e2d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46c5313-3e25-4a33-9641-3b3244ed91c2",
   "metadata": {},
   "source": [
    "### Kodetu interactions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d3b60-aa34-49c8-a30c-ad6f74c5d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kodetu_data = pd.read_csv('kodetuTau-interactions_pre_post.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d85ee40-1512-4d05-b809-66f4e1ded37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows where the 'XMLPseudocode' column contains the word \"EXECUTE\"\n",
    "mask = kodetu_data[\"XMLPseudocode()\"].str.contains('EXECUTE')\n",
    "df_filtered = kodetu_data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a404c5cb-5706-4d19-9f31-53ebc5389f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered[~df_filtered[\"Pseudocode\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3665b2-067d-4412-aeaf-0f7e74411031",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.drop_duplicates() #df_filtered[df_filtered.duplicated(subset=['Pseudocode','Outcome','Level','SessionId'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f913b81-b8c2-4b97-ae63-e23322dbe632",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947446dc-01bc-483a-af64-f4f44ecb9282",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad5456-8031-4b96-bb97-cd0574804b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_level_1 = result #for the basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e2e7fc-530b-4b1d-aa48-3de65c567eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(result_level_1, df, left_on='SessionId', right_on='User', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a9cf71-cbb9-4c32-8a48-d97fc811c07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd5923-31a0-4931-8767-bfa0ddbb6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"Gender\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b4fe34-98c3-4863-a050-356da3fcd36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df  = merged_df[merged_df['Gender']=='1'] #for Male\n",
    "#merged_df  = merged_df[merged_df['Gender']=='2'] #for Female\n",
    "#merged_df  = merged_df[merged_df['Outcome']==1] #for Success\n",
    "#merged_df  = merged_df[merged_df['Outcome']==-1] #for Failure\n",
    "#merged_df  = merged_df[merged_df['Outcome']==-2] #for Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a4f48-39a0-4d97-8c2c-09be0f55932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[['TTCT_PRE_fluency','TTCT_PRE_elaboration','TTCT_PRE_flexibility','TTCT_PRE_originality']] =  merged_df[['TTCT_PRE_fluency','TTCT_PRE_elaboration','TTCT_PRE_flexibility','TTCT_PRE_originality']].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946b4a6-8408-4f48-8276-ca26d6a526bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_values = merged_df[['TTCT_PRE_fluency','TTCT_PRE_elaboration','TTCT_PRE_flexibility','TTCT_PRE_originality']]\n",
    "\n",
    "target_values_normalized = (target_values - target_values.min()) / (target_values.max() - target_values.min())\n",
    "\n",
    "merged_df[['TTCT_PRE_fluency_norm','TTCT_PRE_elaboration_norm','TTCT_PRE_flexibility_norm','TTCT_PRE_originality_norm']] = target_values_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564219b1-8f29-40a6-a4dd-53f7ec3c2df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['num_unique_words'] = merged_df['Pseudocode'].apply(lambda x: len(set(x.split())))\n",
    "merged_df['sequence_length'] = merged_df['Pseudocode'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff422480-30f8-46fa-a231-6412db141479",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[['num_unique_words', 'sequence_length', 'Outcome','Level','Gender']] = merged_df[['num_unique_words', 'sequence_length', 'Outcome','Level','Gender']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f534bc0-63fe-4318-997f-2fcd35fe9008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = merged_df[['num_unique_words', 'sequence_length', 'Outcome','Level','Gender']]\n",
    "y = merged_df[['TTCT_PRE_fluency_norm','TTCT_PRE_flexibility_norm','TTCT_PRE_originality_norm','TTCT_PRE_elaboration_norm']] \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the 4-dimensional vector\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred, multioutput='raw_values'))\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test, y_pred, multioutput='raw_values')\n",
    "print(\"MAE:\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33317c8d-dcc5-49be-b2f8-ab23fbbc9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGBoost model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the 4-dimensional vector\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred, multioutput='raw_values'))\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test, y_pred, multioutput='raw_values')\n",
    "print(\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178566b-8fc2-4e92-a975-b6a3ddab115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code2Creativ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c1d24-f592-4fb1-84e3-e2ededefdc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b171b00c-b0de-4b5c-8e28-d3c81dba0d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e6eb1-c0d9-48bb-8468-ffcdc5349abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9552267-cd4b-4679-b51e-672e86da9293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertRegressionModel(nn.Module):\n",
    "    def __init__(self, bert_model, encoding_dim, regression_hidden_dim):\n",
    "        super(BertRegressionModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Linear(bert_model.config.hidden_size, encoding_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(encoding_dim, regression_hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(regression_hidden_dim, 4)  # Output layer for 4-dimensional regression prediction\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = bert_output.pooler_output\n",
    "        regression_output = self.regression(pooled_output)\n",
    "        return regression_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3389e23-f867-4ae9-96b1-edb90a33ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSequenceDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a354fa98-ffcd-44ff-ac6d-777eeb115da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_indices = X_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb3283-7045-410e-81f6-e55868d717ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_indices = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656ef1b-eb97-4728-b1dd-bad1c3241d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = merged_df[\"Pseudocode\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911d08bb-f36a-431d-9d97-60eebb4d09a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Ensure sequences are a list of strings\n",
    "if isinstance(sequences, pd.Series):\n",
    "    sequences = sequences.tolist()\n",
    "elif isinstance(sequences, np.ndarray):\n",
    "    sequences = sequences.tolist()\n",
    "    \n",
    "# Tokenize sequences\n",
    "tokenized_inputs = tokenizer(sequences, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc98a03-1d39-4978-99f0-bdfbe2ecc4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input_ids and attention_masks\n",
    "input_ids = tokenized_inputs['input_ids']\n",
    "attention_masks = tokenized_inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4488b5d8-2857-4879-ae84-79bcdf5b4ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_indices_np = np.array(Xtrain_indices)\n",
    "Xtest_indices_np = np.array(Xtest_indices)\n",
    "\n",
    "# Use index arrays to get data for training and testing\n",
    "X_train_ids = torch.tensor(input_ids[Xtrain_indices_np], dtype=torch.long)\n",
    "X_test_ids = torch.tensor(input_ids[Xtest_indices_np], dtype=torch.long)\n",
    "\n",
    "X_train_masks = torch.tensor(attention_masks[Xtrain_indices_np], dtype=torch.long)\n",
    "X_test_masks = torch.tensor(attention_masks[Xtest_indices_np], dtype=torch.long)\n",
    "\n",
    "# Convert labels to tensor\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float)\n",
    "\n",
    "# Create training and test datasets\n",
    "train_dataset = BertSequenceDataset(X_train_ids, X_train_masks, y_train_tensor)\n",
    "test_dataset = BertSequenceDataset(X_test_ids, X_test_masks, y_test_tensor)\n",
    "\n",
    "# Create training and test dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Print to confirm everything is set up correctly\n",
    "print(\"Train DataLoader size:\", len(train_dataloader))\n",
    "print(\"Test DataLoader size:\", len(test_dataloader))\n",
    "\n",
    "# Verify the shapes of the tensors\n",
    "print(\"X_train_ids shape:\", X_train_ids.shape)\n",
    "print(\"X_train_masks shape:\", X_train_masks.shape)\n",
    "print(\"y_train_tensor shape:\", y_train_tensor.shape)\n",
    "print(\"X_test_ids shape:\", X_test_ids.shape)\n",
    "print(\"X_test_masks shape:\", X_test_masks.shape)\n",
    "print(\"y_test_tensor shape:\", y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ac22b-d50a-4849-909c-97bf48c48dfa",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9eaf3-e948-4c38-8a45-82f328587e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 32\n",
    "model = BertRegressionModel(bert_model, encoding_dim=256, regression_hidden_dim=32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6436e50d-ebd0-400b-93ac-667f389c3ad4",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d9b573-9085-42f2-a269-2bf11c225f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "clip_value = 1.0  # Clip gradients at this value\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        masks = batch['attention_mask'].to(device)\n",
    "        targets = batch['labels'].float()  # Ensure targets are float for MSELoss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, masks)\n",
    "        loss = criterion(outputs.to(device), targets.to(device))\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    epoch_loss /= len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fea8175-c1a8-4dd2-bdf5-f4a482cbea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            masks = batch['attention_mask'].to(device)\n",
    "            targets = batch['labels'].float() \n",
    "            \n",
    "            outputs = model(inputs, attention_mask=masks)\n",
    "            actuals.extend(targets.cpu().numpy())\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "    \n",
    "    actuals = np.array(actuals)\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    rmse = np.sqrt(np.mean((actuals - predictions) ** 2, axis=0))\n",
    "    mae = np.mean(np.abs(actuals - predictions), axis=0)\n",
    "    r2 = 1 - np.sum((actuals - predictions) ** 2, axis=0) / np.sum((actuals - np.mean(actuals, axis=0)) ** 2, axis=0)\n",
    " \n",
    "    return rmse, mae\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "rmse, mae = evaluate(model, test_dataloader)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77296b27-f926-48d6-92d4-c18b2a29439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HuberLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c75a8e-00ac-4282-8adb-cc59d9186524",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertRegressionModel(bert_model, encoding_dim=128, regression_hidden_dim=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "\n",
    "num_epochs = 10\n",
    "# Clip gradients at this value\n",
    "clip_value = 1.0  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, masks, targets in train_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        masks = masks.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, masks).to(device)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)  \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    epoch_loss /= len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18f334d-cd13-4ab3-bbeb-e9039122d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse, mae = evaluate(model.to(device), test_dataloader)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d4a5d1-9d71-4fb1-9d57-3fb6899aa29d",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f84aea9-8c5a-4831-8aeb-04abbe59367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOWRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim, regression_hidden_dim):\n",
    "        super(BOWRegressionModel, self).__init__()\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(encoding_dim, regression_hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(regression_hidden_dim, 4)  # Output layer for 4-dimensional regression prediction\n",
    "        )\n",
    "\n",
    "    def forward(self, input_bow):\n",
    "        regression_output = self.regression(input_bow)\n",
    "        return regression_output\n",
    "\n",
    "input_dim = len(Xtrain_indices) \n",
    "encoding_dim = 128  \n",
    "regression_hidden_dim = 64 \n",
    "bow_model = BOWRegressionModel(input_dim, encoding_dim, regression_hidden_dim)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(bow_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01775b1a-e740-4252-91bb-34bbd08677dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sequences)\n",
    "\n",
    "\n",
    "X_bow = torch.tensor(X.toarray(), dtype=torch.float)\n",
    "\n",
    "Xtrain_indices_tensor = torch.tensor(Xtrain_indices, dtype=torch.long)\n",
    "Xtest_indices_tensor = torch.tensor(Xtest_indices, dtype=torch.long)\n",
    "\n",
    "X_train_bow = torch.tensor(X_bow[Xtrain_indices_tensor], dtype=torch.float)\n",
    "X_test_bow = torch.tensor(X_bow[Xtest_indices_tensor], dtype=torch.float)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float)\n",
    "\n",
    "train_dataset_bow = TensorDataset(X_train_bow, y_train_tensor)\n",
    "test_dataset_bow = TensorDataset(X_test_bow, y_test_tensor)\n",
    "\n",
    "train_dataloader_bow = DataLoader(train_dataset_bow, batch_size=32, shuffle=True)\n",
    "test_dataloader_bow = DataLoader(test_dataset_bow, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02bfdba-fe05-4327-affe-f0db3474eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "clip_value = 1.0  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    bow_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, targets in train_dataloader_bow:\n",
    "        inputs = inputs\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = bow_model(inputs)\n",
    "        loss = criterion(outputs.to(device), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(bow_model.parameters(), clip_value)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    epoch_loss /= len(train_dataloader_bow)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b52694-6c51-48b0-904c-780208efc215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            #inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            actuals.extend(targets.cpu().numpy())  \n",
    "            predictions.extend(outputs.cpu().numpy()) \n",
    "    \n",
    "    actuals = np.array(actuals)\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    rmse = np.sqrt(np.mean((actuals - predictions) ** 2, axis=0))\n",
    "    mae = np.mean(np.abs(actuals - predictions), axis=0)\n",
    "    r2 = 1 - np.sum((actuals - predictions) ** 2, axis=0) / np.sum((actuals - np.mean(actuals, axis=0)) ** 2, axis=0)\n",
    " \n",
    "    return rmse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f3010-fb59-46c1-8678-7c5eb3bed7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse, mae = evaluate(bow_model, test_dataloader_bow)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf17f7fa-e81e-4bc8-965e-92375224d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7520e6c1-9fa0-42a1-a200-14988790f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a20431-a902-443e-b0c9-d04102b11c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b2126-e107-4466-84c7-991073dc4020",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "clip_value = 1.0  # Clip gradients at this value\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "actual_targets = merged_df[['TTCT_PRE_fluency_norm','TTCT_PRE_flexibility_norm','TTCT_PRE_originality_norm','TTCT_PRE_elaboration_norm']].values\n",
    "num_sequences = len(actual_targets)  # Number of sequences\n",
    "max_seq_length = max(len(seq) for seq in sequences)\n",
    "seq_length = max_seq_length       # Length of each sequence\n",
    "embedding_dim = 20    # Dimension of each embedding\n",
    "output_dim = 4        # Dimension of the target output\n",
    "\n",
    "# Generate random sequences from a normal distribution\n",
    "random_sequences = np.random.randn(num_sequences, seq_length, embedding_dim)\n",
    "#random_input = torch.randint(0, vocab_size, (batch_size, seq_length), dtype=torch.long)\n",
    "\n",
    "X_train = random_sequences[Xtrain_indices_np]\n",
    "X_test = random_sequences[Xtest_indices_np]\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, y_train_tensor)\n",
    "test_dataset = SequenceDataset(X_test, y_test_tensor)\n",
    "\n",
    "# Create training and test dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = GRUAutoencoderWithRegression(vocab_size, embedding_dim, encoding_dim, regression_hidden_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b826f04a-60a4-49d3-a008-d9881b2d9b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, targets in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.float())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    epoch_loss /= len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90f7a69-030e-41e1-8f76-cfeef421f880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs.float())\n",
    "            actuals.append(targets.numpy())\n",
    "            predictions.append(outputs.numpy())\n",
    "    \n",
    "    actuals = np.vstack(actuals)\n",
    "    predictions = np.vstack(predictions)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions, multioutput='raw_values'))\n",
    "    mae = mean_absolute_error(actuals, predictions, multioutput='raw_values')\n",
    "    r2 = r2_score(actuals, predictions, multioutput='raw_values')\n",
    "    \n",
    "    return rmse, mae, r2\n",
    "\n",
    "# Evaluate the model\n",
    "rmse, mae, r2 = evaluate(model, test_dataloader)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R²: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc56d99-48cc-4af6-8810-6368cd99abd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89320473-c771-4a90-b4ce-0f657a096147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceRegressor(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_sequence_length, units, output_dim=4):\n",
    "        super(SequenceRegressor, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, units, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(units, units, batch_first=True)\n",
    "        self.fc = nn.Linear(units, output_dim)\n",
    "\n",
    "    def forward(self, input_sequences):\n",
    "        embedded = self.embedding(input_sequences)\n",
    "        output, (hidden, cell) = self.lstm1(embedded)\n",
    "        output, (hidden, cell) = self.lstm2(output)\n",
    "        output = hidden[-1, :, :]  # Take the last hidden state\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "units = 64\n",
    "\n",
    "model = SequenceRegressor(len(vocab) + 1, embedding_dim, max_seq_length, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba241d8-f81f-4d4b-a991-6c895e5bc745",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b7764-bf4e-437f-8879-ebdaa8441c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "clip_value = 1.0  # Clip gradients at this value\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.float())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    epoch_loss /= len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d0e4e-a3a4-4230-9821-09b497b56b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            actuals.append(targets.numpy())\n",
    "            predictions.append(outputs.numpy())\n",
    "    \n",
    "    actuals = np.vstack(actuals)\n",
    "    predictions = np.vstack(predictions)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions, multioutput='raw_values'))\n",
    "    mae = mean_absolute_error(actuals, predictions, multioutput='raw_values')\n",
    "    \n",
    "    return rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18fd39b-e672-4495-9ed9-9a1367fcd2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "rmse, mae  = evaluate(model, dataloader)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
